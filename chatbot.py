import os
import PyPDF2
import json
from sentence_transformers import SentenceTransformer, util
from transformers import pipeline
import streamlit as st

# === 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† PDF (Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ù…Ù„Ù ÙˆØ§Ø­Ø¯ ÙÙ‚Ø· Ù„Ù„ØªØ¬Ø±Ø¨Ø©) ===

def extract_text_from_pdf(pdf_path):
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        full_text = ''
        for page in reader.pages:
            text = page.extract_text()
            if text:
                full_text += text + '\n'
        return full_text

# === 2. ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙØµÙˆÙ„ Ø£Ùˆ ÙˆØ­Ø¯Ø§Øª (Ù„ØªØ³Ù‡ÙŠÙ„ Ø§Ù„Ø¨Ø­Ø«) ===

def split_into_units(text, chunk_size=500):
    words = text.split()
    chunks = [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]
    return chunks

# === 3. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ===

@st.cache_resource
def load_models():
    search_model = SentenceTransformer('all-MiniLM-L6-v2')
    qa_pipeline = pipeline("question-answering", model="distilbert-base-cased-distilled-squad")
    return search_model, qa_pipeline

search_model, qa_pipeline = load_models()

# === 4. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ===

st.title("ðŸ¤– Medical Chatbot - Hybrid Mode")
st.markdown("Ask your medical question and get a smart answer.")

user_input = st.text_input("Enter your question...")

# === 5. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„ ===

if user_input:
    # ðŸ” Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù†Øµ Ù…Ù† PDF (Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ ÙƒØªØ§Ø¨ ÙˆØ§Ø­Ø¯)
    pdf_file = "example_book.pdf"  # Ø§Ø³ØªØ¨Ø¯Ù„ Ø¨Ø§Ø³Ù… ÙƒØªØ§Ø¨Ùƒ
    if not os.path.exists(pdf_file):
        st.error(f"âŒ File not found: {pdf_file}")
    else:
        with st.spinner("ðŸ” Extracting text from PDF..."):
            book_text = extract_text_from_pdf(pdf_file)

        # ðŸ“– Ø§Ù„Ø®Ø·ÙˆØ© 2: ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙˆØ­Ø¯Ø§Øª ØµØºÙŠØ±Ø©
        chunks = split_into_units(book_text)

        # ðŸ” Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø§Ù„Ø¨Ø­Ø« Ù…Ø¹ Semantic Search
        chunk_embeddings = search_model.encode(chunks, convert_to_tensor=True)
        question_embedding = search_model.encode(user_input, convert_to_tensor=True)
        scores = util.cos_sim(question_embedding, chunk_embeddings)[0]
        best_chunk_index = scores.argmax().item()
        best_chunk = chunks[best_chunk_index]

        st.markdown("### ðŸ“ Best Matching Text Found:")
        st.info(best_chunk[:500] + ("..." if len(best_chunk) > 500 else ""))

        # ðŸ’¬ Ø§Ù„Ø®Ø·ÙˆØ© 4: Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Transformers
        with st.spinner("ðŸ§  Generating answer..."):
            result = qa_pipeline(question=user_input, context=best_chunk)

        st.markdown("### âœ… Smart Answer Generated:")
        st.success(result['answer'])
