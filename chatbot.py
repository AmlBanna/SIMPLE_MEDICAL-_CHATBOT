import json
import re
from sentence_transformers import SentenceTransformer, util
import streamlit as st
import os

# Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙ†Ø§ ÙÙŠ Ø§Ù„ØªØµØ­ÙŠØ­
st.write("ðŸ“‚ Current Working Directory:", os.getcwd())
st.write("ðŸ“„ Files in current directory:", os.listdir())

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„Ù
file_path = "medical_knowledge_with_keywords.json"
if not os.path.exists(file_path):
    st.error(f"âŒ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {file_path}")
    st.stop()
else:
    st.success(f"âœ… Ø§Ù„Ù…Ù„Ù Ù…ÙˆØ¬ÙˆØ¯: {file_path}")

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ø³ÙŠØªÙ… ØªÙ†Ø²ÙŠÙ„Ù‡ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¹Ù†Ø¯ Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø£ÙˆÙ„)
@st.cache_resource
def load_model():
    return SentenceTransformer('all-MiniLM-L6-v2')

model = load_model()

# ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
with open(file_path, 'r', encoding='utf-8') as f:
    knowledge_base = json.load(f)

# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† ÙˆØ§Ù„Ù…Ù„Ø®ØµØ§Øª Ø¥Ù„Ù‰ ØªØ¹Ø¨ÙŠØ±Ø§Øª Ù…Ø¶Ù…Ù†Ø©
passages = [item['title'] + " " + item.get('summary', '') for item in knowledge_base]
embeddings = model.encode(passages, convert_to_tensor=True)

def find_best_matches(question, top_k=3):
    question_emb = model.encode(question, convert_to_tensor=True)
    hits = util.semantic_search(question_emb, embeddings, top_k=top_k)[0]

    results = []
    for hit in hits:
        result = knowledge_base[hit['corpus_id']]
        result['score'] = hit['score']
        results.append(result)
    return results

# ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
st.title("ðŸ¤– Ø§Ù„Ø´Ø§Øª Ø¨ÙˆØª Ø§Ù„Ø·Ø¨ÙŠ")
st.markdown("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø§Ù„Ø·Ø¨ÙŠ ÙˆØ³ÙŠÙ‚ÙˆÙ… Ø§Ù„Ø¨ÙˆØª Ø¨Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù…Ù† Ø§Ù„ÙƒØªØ¨.")

user_input = st.text_input("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§...")

# Ø®ÙŠØ§Ø± Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ÙØ¦Ø©
category_filter = st.selectbox(
    "Ø§Ø®ØªØ± ÙØ¦Ø© Ù„Ù„Ø¨Ø­Ø« Ø¯Ø§Ø®Ù„Ù‡Ø§ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ):",
    ["ÙƒÙ„ Ø§Ù„ÙØ¦Ø§Øª", "pharmacology", "first_aid", "pain_management"]
)

if user_input:
    results = find_best_matches(user_input, top_k=3)

    if category_filter != "ÙƒÙ„ Ø§Ù„ÙØ¦Ø§Øª":
        results = [r for r in results if r.get('category') == category_filter]

    if results:
        st.subheader("ðŸ” Ø£ÙØ¶Ù„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬:")
        for i, result in enumerate(results):
            with st.expander(f"ðŸ† Ø§Ù„Ù†ØªÙŠØ¬Ø© #{i+1} - Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡: {result['score']:.2f}"):
                st.markdown(f"**Ø§Ù„Ø¹Ù†ÙˆØ§Ù†:** {result['title']}")
                st.markdown(f"**Ø§Ù„ÙØ¦Ø©:** {result.get('category', '-')}")
                st.markdown(f"**Ø§Ù„Ù…Ù„Ø®Øµ:** {result.get('summary', 'Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ù„Ø®Øµ')}")

                # Ø¹Ø±Ø¶ Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ Ø£Ùˆ Ø¬Ø²Ø¡ Ù…Ù†Ù‡
                content = result.get('content', '')
                if len(content) > 500:
                    st.markdown(f"**Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ù†Øµ:** {content[:500]}...")
                else:
                    st.markdown(f"**Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„:** {content}")
    else:
        st.warning("Ø¹Ø°Ø±Ù‹Ø§ØŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ØªØ§Ø­Ø© Ù„Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„.")
